# ETF/Index/Commodity AI Signals (Direct)
# ========================================
# Processes ETFs, Indices, and Commodities using Gemini AI directly in GitHub Actions.
# Simpler than E2B version - runs sequentially but more reliable.
#
# TRIGGERS:
# - Manual dispatch
# - After setup scanner completes

name: ETF/Index/Commodity AI Signals (Direct)

on:
  workflow_run:
    workflows: ["ETF/Index/Commodity Daily Setup Scanner"]
    types:
      - completed
  
  workflow_dispatch:
    inputs:
      target_date:
        description: 'Target date (YYYY-MM-DD). Leave empty for today.'
        required: false
        type: string
      batch_size:
        description: 'Number of assets per batch (default: 10)'
        required: false
        default: '10'
        type: string
      model:
        description: 'Gemini model to use'
        required: false
        default: 'gemini-3-flash-preview'
        type: string

env:
  PYTHON_VERSION: '3.11'

jobs:
  ai-analysis:
    name: ETF/Index/Commodity AI Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install psycopg2-binary google-genai
      
      - name: Determine target date
        id: date
        run: |
          if [ -n "${{ github.event.inputs.target_date }}" ]; then
            echo "date=${{ github.event.inputs.target_date }}" >> $GITHUB_OUTPUT
          else
            echo "date=$(date -u +%Y-%m-%d)" >> $GITHUB_OUTPUT
          fi
      
      - name: Run AI analysis
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          echo "üöÄ Starting ETF/Index/Commodity AI Analysis (Direct)"
          echo "üìÖ Date: ${{ steps.date.outputs.date }}"
          echo "üî¢ Batch Size: ${{ github.event.inputs.batch_size || '10' }}"
          echo "ü§ñ Model: ${{ github.event.inputs.model || 'gemini-3-flash-preview' }}"
          echo ""
          
          # Get asset count
          python3 -c "
import psycopg2
import os

db_url = os.environ['DATABASE_URL']
conn = psycopg2.connect(db_url)
cur = conn.cursor()
cur.execute(\"SELECT COUNT(*) FROM assets WHERE asset_type IN ('etf', 'index', 'commodity') AND is_active = TRUE\")
count = cur.fetchone()[0]
print(f'Total ETF/Index/Commodity assets to process: {count}')
conn.close()
"
          
          # Process in batches
          offset=0
          batch_size=${{ github.event.inputs.batch_size || '10' }}
          total_processed=0
          
          while true; do
            echo "Processing batch at offset $offset (batch size: $batch_size)..."
            
            python3 scripts/run_etf_ai_analysis_batch.py \
              --date ${{ steps.date.outputs.date }} \
              --model ${{ github.event.inputs.model || 'gemini-3-flash-preview' }} \
              --offset $offset \
              --batch-size $batch_size
            
            exit_code=$?
            
            if [ $exit_code -ne 0 ]; then
              echo "‚ö†Ô∏è Batch at offset $offset failed with exit code $exit_code"
            else
              total_processed=$((total_processed + batch_size))
              echo "‚úÖ Batch completed"
            fi
            
            offset=$((offset + batch_size))
            
            # Stop after processing 180 assets (safety limit for ETF+Index+Commodity)
            if [ $offset -ge 180 ]; then
              echo "Reached safety limit (180 assets)"
              break
            fi
            
            # Small delay between batches
            sleep 5
          done
          
          echo ""
          echo "‚úÖ ETF/Index/Commodity AI analysis completed"
          echo "Total assets attempted: $total_processed"
      
      - name: Report results
        run: |
          echo "üìä ETF/Index/Commodity AI Analysis Summary"
          echo "=========================================="
          
          # Count reviews created
          python3 -c "
import psycopg2
import os
from datetime import datetime

db_url = os.environ['DATABASE_URL']
conn = psycopg2.connect(db_url)
cur = conn.cursor()

target_date = '${{ steps.date.outputs.date }}'

# Count reviews by asset type
cur.execute('''
    SELECT a.asset_type, COUNT(*) FROM asset_ai_reviews aar
    JOIN assets a ON aar.asset_id = a.asset_id
    WHERE aar.as_of_date = %s AND a.asset_type IN ('etf', 'index', 'commodity')
    GROUP BY a.asset_type
''', (target_date,))

print('AI reviews created by type:')
for row in cur.fetchall():
    print(f'  {row[0]}: {row[1]}')

# Show top by direction score
cur.execute('''
    SELECT a.symbol, a.asset_type, aar.ai_attention_level, aar.ai_direction_score
    FROM asset_ai_reviews aar
    JOIN assets a ON aar.asset_id = a.asset_id
    WHERE aar.as_of_date = %s AND a.asset_type IN ('etf', 'index', 'commodity')
    ORDER BY aar.ai_direction_score DESC NULLS LAST
    LIMIT 10
''', (target_date,))

rows = cur.fetchall()
if rows:
    print('\nTop 10 by direction score:')
    for row in rows:
        print(f'  {row[0]} ({row[1]}): {row[2]} (score: {row[3]})')

conn.close()
" || echo "Could not fetch summary"
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
